{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from scipy import stats\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import gc\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_corr(y_true, y_pred):\n",
    "        if np.ndim(y_pred) == 2:\n",
    "            corr = np.mean([stats.spearmanr(y_true[:, i], y_pred[:, i])[0] for i in range(y_true.shape[1])])\n",
    "        else:\n",
    "            corr = stats.spearmanr(y_true, y_pred)[0]\n",
    "        return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_1 = train['question_body']\n",
    "test_text_1 = test['question_body']\n",
    "all_text_1 = pd.concat([train_text_1, test_text_1])\n",
    "\n",
    "train_text_2 = train['answer']\n",
    "test_text_2 = test['answer']\n",
    "all_text_2 = pd.concat([train_text_2, test_text_2])\n",
    "\n",
    "train_text_3 = train['question_title']\n",
    "test_text_3 = test['question_title']\n",
    "all_text_3 = pd.concat([train_text_3, test_text_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\feature_extraction\\text.py:520: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=80000)\n",
    "word_vectorizer.fit(all_text_1)\n",
    "train_word_features_1 = word_vectorizer.transform(train_text_1)\n",
    "test_word_features_1 = word_vectorizer.transform(test_text_1)\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=80000)\n",
    "word_vectorizer.fit(all_text_2)\n",
    "train_word_features_2 = word_vectorizer.transform(train_text_2)\n",
    "test_word_features_2 = word_vectorizer.transform(test_text_2)\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=80000)\n",
    "word_vectorizer.fit(all_text_3)\n",
    "train_word_features_3 = word_vectorizer.transform(train_text_3)\n",
    "test_word_features_3 = word_vectorizer.transform(test_text_3)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=47000)\n",
    "char_vectorizer.fit(all_text_1)\n",
    "train_char_features_1 = char_vectorizer.transform(train_text_1)\n",
    "test_char_features_1 = char_vectorizer.transform(test_text_1)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=47000)\n",
    "char_vectorizer.fit(all_text_2)\n",
    "train_char_features_2 = char_vectorizer.transform(train_text_2)\n",
    "test_char_features_2 = char_vectorizer.transform(test_text_2)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=47000)\n",
    "char_vectorizer.fit(all_text_3)\n",
    "train_char_features_3 = char_vectorizer.transform(train_text_3)\n",
    "test_char_features_3 = char_vectorizer.transform(test_text_3)\n",
    "\n",
    "train_features_1 = hstack([train_char_features_1, train_word_features_1, train_char_features_3, train_word_features_3])\n",
    "test_features_1 = hstack([test_char_features_1, test_word_features_1, test_char_features_3, test_word_features_3])\n",
    "train_features_2 = hstack([train_char_features_2, train_word_features_2])\n",
    "test_features_2 = hstack([test_char_features_2, test_word_features_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_1= train_features_1.tocsr()\n",
    "train_features_2= train_features_2.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {'question_asker_intent_understanding': 40,\n",
    "         'question_body_critical':7,\n",
    "          'question_conversational':35,\n",
    "          'question_expect_short_answer':65,\n",
    "          'question_fact_seeking':10,\n",
    "          'question_has_commonly_accepted_answer':25,\n",
    "          'question_interestingness_others':50,\n",
    "          'question_interestingness_self':30,\n",
    "          'question_multi_intent':7,\n",
    "          'question_not_really_a_question':55,\n",
    "          'question_opinion_seeking':15,\n",
    "          'question_type_choice':4,\n",
    "          'question_type_compare':30,\n",
    "          'question_type_consequence':45,\n",
    "          'question_type_definition':60,\n",
    "          'question_type_entity':11,\n",
    "          'question_type_instructions':6,\n",
    "          'question_type_procedure':40,\n",
    "          'question_type_reason_explanation':13,\n",
    "          'question_type_spelling':1,\n",
    "          'question_well_written':8,\n",
    "          'answer_helpful':30,\n",
    "          'answer_level_of_information':8,\n",
    "          'answer_plausible':20,\n",
    "          'answer_relevance':60,\n",
    "          'answer_satisfaction':11,\n",
    "          'answer_type_instructions':3,\n",
    "          'answer_type_procedure':25,\n",
    "          'answer_type_reason_explanation':3,\n",
    "          'answer_well_written':25\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv').fillna(' ')\n",
    "class_names = list(sample_submission.columns[1:])\n",
    "class_names_q = class_names[:21]\n",
    "class_names_a = class_names[21:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3e9198359342aa88aab8be8f140b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_asker_intent_understanding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman_corr: 0.353260647741117\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'question_asker_intent_understanding_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'question_asker_intent_understanding_2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9a43caabed58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spearman_corr:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspearman_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mspearman_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspearman_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_oof_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"auc:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mtrain_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_oof_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'question_asker_intent_understanding_2'"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame.from_dict({'qa_id': test['qa_id']})\n",
    "\n",
    "train_preds = []\n",
    "test_preds = []\n",
    "scores = []\n",
    "spearman_scores = []\n",
    "\n",
    "for class_name in tqdm_notebook(class_names_q):\n",
    "    print(class_name)\n",
    "    Y = train[class_name]\n",
    "    \n",
    "    n_splits = 3\n",
    "    kf = KFold(n_splits=n_splits, random_state=47)\n",
    "\n",
    "    train_oof_1 = np.zeros((train_features_1.shape[0], ))\n",
    "    test_preds_1 = 0\n",
    "    \n",
    "    score = 0\n",
    "\n",
    "    for jj, (train_index, val_index) in enumerate(kf.split(train_features_1)):\n",
    "        #print(\"Fitting fold\", jj+1)\n",
    "        train_features = train_features_1[train_index]\n",
    "        train_target = Y[train_index]\n",
    "\n",
    "        val_features = train_features_1[val_index]\n",
    "        val_target = Y[val_index]\n",
    "\n",
    "        model = Ridge(alpha = alphas[class_name])\n",
    "        model.fit(train_features, train_target)\n",
    "        val_pred = model.predict(val_features)\n",
    "        train_oof_1[val_index] = val_pred\n",
    "        #print(\"Fold auc:\", roc_auc_score(val_target, val_pred))\n",
    "        #spearman_corr\n",
    "        #score += roc_auc_score(val_target, val_pred)/n_splits\n",
    "\n",
    "        test_preds_1 += model.predict(test_features_1)/n_splits\n",
    "        del train_features, train_target, val_features, val_target\n",
    "        gc.collect()\n",
    "        \n",
    "    model = Ridge(alpha = alphas[class_name])\n",
    "    model.fit(train_features_1, Y)\n",
    "    #print(test_preds_1.max())\n",
    "    #print(test_preds_1.min())\n",
    "    mms = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "    test_preds_1 = mms.fit_transform(test_preds_1.reshape(-1, 1)).flatten()\n",
    "    preds = model.predict(test_features_1)\n",
    "    mms = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "    preds = mms.fit_transform(preds.reshape(-1, 1)).flatten()\n",
    "    submission[class_name] = (0.75*test_preds_1+0.25*preds+0.000005)/1.00001\n",
    "    spearman_score = spearman_corr(train[class_name], train_oof_1)\n",
    "    print(\"spearman_corr:\", spearman_score) \n",
    "    spearman_scores.append(spearman_score)\n",
    "    score = roc_auc_score(train[class_name+'_2'], train_oof_1)    \n",
    "    print(\"auc:\", score, \"\\n\")\n",
    "    train_preds.append(train_oof_1)\n",
    "    test_preds.append(test_preds_1)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for class_name in tqdm_notebook(class_names_a):\n",
    "    print(class_name)\n",
    "    Y = train[class_name]\n",
    "    \n",
    "    n_splits = 3\n",
    "    kf = KFold(n_splits=n_splits, random_state=47)\n",
    "\n",
    "    train_oof_2 = np.zeros((train_features_2.shape[0], ))\n",
    "    test_preds_2 = 0\n",
    "    \n",
    "    score = 0\n",
    "\n",
    "    for jj, (train_index, val_index) in enumerate(kf.split(train_features_1)):\n",
    "        #print(\"Fitting fold\", jj+1)\n",
    "        train_features = train_features_2[train_index]\n",
    "        train_target = Y[train_index]\n",
    "\n",
    "        val_features = train_features_2[val_index]\n",
    "        val_target = Y[val_index]\n",
    "\n",
    "        model = Ridge(alpha = alphas[class_name])\n",
    "        model.fit(train_features, train_target)\n",
    "        val_pred = model.predict(val_features)\n",
    "        train_oof_2[val_index] = val_pred\n",
    "        #print(\"Fold auc:\", roc_auc_score(val_target, val_pred))\n",
    "        #score += roc_auc_score(val_target, val_pred)/n_splits\n",
    "\n",
    "        test_preds_2 += model.predict(test_features_2)/n_splits\n",
    "        del train_features, train_target, val_features, val_target\n",
    "        gc.collect()\n",
    "        \n",
    "    model = Ridge(alpha = alphas[class_name])\n",
    "    model.fit(train_features_2, Y)\n",
    "    print(test_preds_2.max())\n",
    "    \n",
    "    preds = model.predict(test_features_2)\n",
    "    mms = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "    test_preds_2 = mms.fit_transform(test_preds_2.reshape(-1, 1)).flatten()\n",
    "    mms = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "    preds = mms.fit_transform(preds.reshape(-1, 1)).flatten()\n",
    "    submission[class_name] = (0.75*test_preds_2+0.25*preds+0.000005)/1.00001\n",
    "        \n",
    "    score = roc_auc_score(train[class_name+'_2'], train_oof_2) \n",
    "    \n",
    "    \n",
    "    spearman_score = spearman_corr(train[class_name], train_oof_2)\n",
    "    print(\"spearman_corr:\", spearman_score)\n",
    "    print(\"auc:\", score, \"\\n\")\n",
    "    spearman_scores.append(spearman_score)\n",
    "    \n",
    "    train_preds.append(train_oof_2)\n",
    "    test_preds.append(test_preds_2)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean auc:\", np.mean(scores))\n",
    "print(\"Mean spearman_scores\", np.mean(spearman_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
