{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"190103_StratifiedKFold_Emmy.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"NKFzPO12YUb7","colab_type":"code","colab":{}},"source":["# #Import packages\n","# !pip install tqdm\n","# !pip install bert-tensorflow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5__jeX1YXC7a","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import GroupKFold\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm_notebook, tqdm\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","#import bert.tokenization as tokenization\n","import tensorflow.keras.backend as K\n","import gc\n","import os\n","from scipy.stats import spearmanr\n","from math import floor, ceil\n","from tensorflow.keras.models import load_model\n","\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","np.set_printoptions(suppress=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u2OxRk0FXC7h","colab_type":"text"},"source":["The following cell is for bert_tokenization package"]},{"cell_type":"code","metadata":{"id":"Yuutj5gtXC7j","colab_type":"code","colab":{}},"source":["import collections\n","import re\n","import unicodedata\n","import six\n","import tensorflow as tf\n","\n","\n","def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n","    \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n","\n","    # The casing has to be passed in by the user and there is no explicit check\n","    # as to whether it matches the checkpoint. The casing information probably\n","    # should have been stored in the bert_config.json file, but it's not, so\n","    # we have to heuristically detect it to validate.\n","\n","    if not init_checkpoint:\n","        return\n","\n","    m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n","    if m is None:\n","        return\n","\n","    model_name = m.group(1)\n","\n","    lower_models = [\n","        \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n","        \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n","    ]\n","\n","    cased_models = [\n","        \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n","        \"multi_cased_L-12_H-768_A-12\"\n","    ]\n","\n","    is_bad_config = False\n","    if model_name in lower_models and not do_lower_case:\n","        is_bad_config = True\n","        actual_flag = \"False\"\n","        case_name = \"lowercased\"\n","        opposite_flag = \"True\"\n","\n","    if model_name in cased_models and do_lower_case:\n","        is_bad_config = True\n","        actual_flag = \"True\"\n","        case_name = \"cased\"\n","        opposite_flag = \"False\"\n","\n","    if is_bad_config:\n","        raise ValueError(\n","            \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n","            \"However, `%s` seems to be a %s model, so you \"\n","            \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n","            \"how the model was pre-training. If this error is wrong, please \"\n","            \"just comment out this check.\" % (actual_flag, init_checkpoint,\n","                                              model_name, case_name, opposite_flag))\n","\n","\n","def convert_to_unicode(text):\n","    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n","    if six.PY3:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, bytes):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    elif six.PY2:\n","        if isinstance(text, str):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        elif isinstance(text, unicode):\n","            return text\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    else:\n","        raise ValueError(\"Not running on Python2 or Python 3?\")\n","\n","\n","def printable_text(text):\n","    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n","\n","    # These functions want `str` for both Python2 and Python3, but in one case\n","    # it's a Unicode string and in the other it's a byte string.\n","    if six.PY3:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, bytes):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    elif six.PY2:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, unicode):\n","            return text.encode(\"utf-8\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    else:\n","        raise ValueError(\"Not running on Python2 or Python 3?\")\n","\n","\n","def load_vocab(vocab_file):\n","    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n","    vocab = collections.OrderedDict()\n","    index = 0\n","    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n","        while True:\n","            token = convert_to_unicode(reader.readline())\n","            if not token:\n","                break\n","            token = token.strip()\n","            vocab[token] = index\n","            index += 1\n","    return vocab\n","\n","\n","def convert_by_vocab(vocab, items):\n","    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n","    output = []\n","    for item in items:\n","        output.append(vocab[item])\n","    return output\n","\n","\n","def convert_tokens_to_ids(vocab, tokens):\n","    return convert_by_vocab(vocab, tokens)\n","\n","\n","def convert_ids_to_tokens(inv_vocab, ids):\n","    return convert_by_vocab(inv_vocab, ids)\n","\n","\n","def whitespace_tokenize(text):\n","    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n","    text = text.strip()\n","    if not text:\n","        return []\n","    tokens = text.split()\n","    return tokens\n","\n","\n","class FullTokenizer(object):\n","    \"\"\"Runs end-to-end tokenziation.\"\"\"\n","\n","    def __init__(self, vocab_file, do_lower_case=True):\n","        self.vocab = load_vocab(vocab_file)\n","        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n","        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n","\n","    def tokenize(self, text):\n","        split_tokens = []\n","        for token in self.basic_tokenizer.tokenize(text):\n","            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n","                split_tokens.append(sub_token)\n","\n","        return split_tokens\n","\n","    def convert_tokens_to_ids(self, tokens):\n","        return convert_by_vocab(self.vocab, tokens)\n","\n","    def convert_ids_to_tokens(self, ids):\n","        return convert_by_vocab(self.inv_vocab, ids)\n","\n","\n","class BasicTokenizer(object):\n","    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n","\n","    def __init__(self, do_lower_case=True):\n","        \"\"\"Constructs a BasicTokenizer.\n","        Args:\n","          do_lower_case: Whether to lower case the input.\n","        \"\"\"\n","        self.do_lower_case = do_lower_case\n","\n","    def tokenize(self, text):\n","        \"\"\"Tokenizes a piece of text.\"\"\"\n","        text = convert_to_unicode(text)\n","        text = self._clean_text(text)\n","\n","        # This was added on November 1st, 2018 for the multilingual and Chinese\n","        # models. This is also applied to the English models now, but it doesn't\n","        # matter since the English models were not trained on any Chinese data\n","        # and generally don't have any Chinese data in them (there are Chinese\n","        # characters in the vocabulary because Wikipedia does have some Chinese\n","        # words in the English Wikipedia.).\n","        text = self._tokenize_chinese_chars(text)\n","\n","        orig_tokens = whitespace_tokenize(text)\n","        split_tokens = []\n","        for token in orig_tokens:\n","            if self.do_lower_case:\n","                token = token.lower()\n","                token = self._run_strip_accents(token)\n","            split_tokens.extend(self._run_split_on_punc(token))\n","\n","        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n","        return output_tokens\n","\n","    def _run_strip_accents(self, text):\n","        \"\"\"Strips accents from a piece of text.\"\"\"\n","        text = unicodedata.normalize(\"NFD\", text)\n","        output = []\n","        for char in text:\n","            cat = unicodedata.category(char)\n","            if cat == \"Mn\":\n","                continue\n","            output.append(char)\n","        return \"\".join(output)\n","\n","    def _run_split_on_punc(self, text):\n","        \"\"\"Splits punctuation on a piece of text.\"\"\"\n","        chars = list(text)\n","        i = 0\n","        start_new_word = True\n","        output = []\n","        while i < len(chars):\n","            char = chars[i]\n","            if _is_punctuation(char):\n","                output.append([char])\n","                start_new_word = True\n","            else:\n","                if start_new_word:\n","                    output.append([])\n","                start_new_word = False\n","                output[-1].append(char)\n","            i += 1\n","\n","        return [\"\".join(x) for x in output]\n","\n","    def _tokenize_chinese_chars(self, text):\n","        \"\"\"Adds whitespace around any CJK character.\"\"\"\n","        output = []\n","        for char in text:\n","            cp = ord(char)\n","            if self._is_chinese_char(cp):\n","                output.append(\" \")\n","                output.append(char)\n","                output.append(\" \")\n","            else:\n","                output.append(char)\n","        return \"\".join(output)\n","\n","    def _is_chinese_char(self, cp):\n","        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n","        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n","        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n","        #\n","        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n","        # despite its name. The modern Korean Hangul alphabet is a different block,\n","        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n","        # space-separated words, so they are not treated specially and handled\n","        # like the all of the other languages.\n","        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n","                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n","                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n","                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n","                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n","                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n","                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n","                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n","            return True\n","\n","        return False\n","\n","    def _clean_text(self, text):\n","        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n","        output = []\n","        for char in text:\n","            cp = ord(char)\n","            if cp == 0 or cp == 0xfffd or _is_control(char):\n","                continue\n","            if _is_whitespace(char):\n","                output.append(\" \")\n","            else:\n","                output.append(char)\n","        return \"\".join(output)\n","\n","\n","class WordpieceTokenizer(object):\n","    \"\"\"Runs WordPiece tokenziation.\"\"\"\n","\n","    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n","        self.vocab = vocab\n","        self.unk_token = unk_token\n","        self.max_input_chars_per_word = max_input_chars_per_word\n","\n","    def tokenize(self, text):\n","        \"\"\"Tokenizes a piece of text into its word pieces.\n","        This uses a greedy longest-match-first algorithm to perform tokenization\n","        using the given vocabulary.\n","        For example:\n","          input = \"unaffable\"\n","          output = [\"un\", \"##aff\", \"##able\"]\n","        Args:\n","          text: A single token or whitespace separated tokens. This should have\n","            already been passed through `BasicTokenizer.\n","        Returns:\n","          A list of wordpiece tokens.\n","        \"\"\"\n","\n","        text = convert_to_unicode(text)\n","\n","        output_tokens = []\n","        for token in whitespace_tokenize(text):\n","            chars = list(token)\n","            if len(chars) > self.max_input_chars_per_word:\n","                output_tokens.append(self.unk_token)\n","                continue\n","\n","            is_bad = False\n","            start = 0\n","            sub_tokens = []\n","            while start < len(chars):\n","                end = len(chars)\n","                cur_substr = None\n","                while start < end:\n","                    substr = \"\".join(chars[start:end])\n","                    if start > 0:\n","                        substr = \"##\" + substr\n","                    if substr in self.vocab:\n","                        cur_substr = substr\n","                        break\n","                    end -= 1\n","                if cur_substr is None:\n","                    is_bad = True\n","                    break\n","                sub_tokens.append(cur_substr)\n","                start = end\n","\n","            if is_bad:\n","                output_tokens.append(self.unk_token)\n","            else:\n","                output_tokens.extend(sub_tokens)\n","        return output_tokens\n","\n","\n","def _is_whitespace(char):\n","    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n","    # \\t, \\n, and \\r are technically contorl characters but we treat them\n","    # as whitespace since they are generally considered as such.\n","    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n","        return True\n","    cat = unicodedata.category(char)\n","    if cat == \"Zs\":\n","        return True\n","    return False\n","\n","\n","def _is_control(char):\n","    \"\"\"Checks whether `chars` is a control character.\"\"\"\n","    # These are technically control characters but we count them as whitespace\n","    # characters.\n","    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n","        return False\n","    cat = unicodedata.category(char)\n","    if cat in (\"Cc\", \"Cf\"):\n","        return True\n","    return False\n","\n","\n","def _is_punctuation(char):\n","    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n","    cp = ord(char)\n","    # We treat all non-letter/number ASCII as punctuation.\n","    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n","    # Punctuation class but we treat them as punctuation anyways, for\n","    # consistency.\n","    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n","            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n","        return True\n","    cat = unicodedata.category(char)\n","    if cat.startswith(\"P\"):\n","        return True\n","    return False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NSwKilRAXC7l","colab_type":"text"},"source":["# Bert-based Model"]},{"cell_type":"code","metadata":{"id":"KJZS3nduXC7m","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import GroupKFold\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm_notebook, tqdm\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","#import bert_tokenization as tokenization\n","import tensorflow.keras.backend as K\n","import gc\n","import os\n","from scipy.stats import spearmanr\n","from math import floor, ceil\n","from tensorflow.keras.models import load_model\n","\n","np.set_printoptions(suppress=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oy6tOB5paKqs","colab_type":"code","outputId":"8631a3cc-f11a-4344-cc22-a71ea9ca8df5","executionInfo":{"status":"ok","timestamp":1578100317427,"user_tz":300,"elapsed":3781,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# Import data from https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount = True)\n","# there will be a prompt for authentication. Follow the link, click yes, and copy the token\n","\n","# get name of the files in folder\n","!ls \"/content/gdrive/My Drive/GoogleQA/data/\"\n"],"execution_count":59,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","bert  sample_submission.csv  test.csv  train.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-8b7dsvVXC7o","colab_type":"code","outputId":"b70c05a3-060e-4387-9336-7748bfa21243","executionInfo":{"status":"ok","timestamp":1578100318070,"user_tz":300,"elapsed":4407,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":183}},"source":["# Import data\n","PATH = '/content/gdrive/My Drive/GoogleQA/data/'\n","BERT_PATH = PATH +'bert/'\n","tokenizer = FullTokenizer(BERT_PATH+'vocab.txt', True)\n","MAX_SEQUENCE_LENGTH = 512\n","\n","df_train = pd.read_csv(PATH+'train.csv')\n","df_test = pd.read_csv(PATH+'test.csv')\n","df_sub = pd.read_csv(PATH+'sample_submission.csv')\n","print('train shape =', df_train.shape)\n","print('test shape =', df_test.shape)\n","print('sub shape =', df_sub.shape)\n","\n","output_categories = list(df_train.columns[11:])\n","input_categories = list(df_train.columns[[1,2,5]])\n","print('\\noutput categories:\\n\\t', output_categories)\n","print('\\ninput categories:\\n\\t', input_categories)"],"execution_count":60,"outputs":[{"output_type":"stream","text":["train shape = (6079, 41)\n","test shape = (476, 11)\n","sub shape = (476, 31)\n","\n","output categories:\n","\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n","\n","input categories:\n","\t ['question_title', 'question_body', 'answer']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PX03u1B1XC7s","colab_type":"code","colab":{}},"source":["def _get_masks(tokens, max_seq_length):\n","    \"\"\"Mask for padding\"\"\"\n","    if len(tokens)>max_seq_length:\n","        raise IndexError(\"Token length more than max seq length!\")\n","    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n","\n","def _get_segments(tokens, max_seq_length):\n","    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n","    if len(tokens)>max_seq_length:\n","        raise IndexError(\"Token length more than max seq length!\")\n","    segments = []\n","    first_sep = True\n","    current_segment_id = 0\n","    for token in tokens:\n","        segments.append(current_segment_id)\n","        if token == \"[SEP]\":\n","            if first_sep:\n","                first_sep = False \n","            else:\n","                current_segment_id = 1\n","    return segments + [0] * (max_seq_length - len(tokens))\n","\n","def _get_ids(tokens, tokenizer, max_seq_length):\n","    \"\"\"Token ids from Tokenizer vocab\"\"\"\n","    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n","    return input_ids\n","\n","def _trim_input(title, question, answer, max_sequence_length, \n","                t_max_len=30, q_max_len=239, a_max_len=239):\n","\n","    t = tokenizer.tokenize(title)\n","    q = tokenizer.tokenize(question)\n","    a = tokenizer.tokenize(answer)\n","    \n","    t_len = len(t)\n","    q_len = len(q)\n","    a_len = len(a)\n","\n","    if (t_len+q_len+a_len+4) > max_sequence_length:\n","        \n","        if t_max_len > t_len:\n","            t_new_len = t_len\n","            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n","            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n","        else:\n","            t_new_len = t_max_len\n","      \n","        if a_max_len > a_len:\n","            a_new_len = a_len \n","            q_new_len = q_max_len + (a_max_len - a_len)\n","        elif q_max_len > q_len:\n","            a_new_len = a_max_len + (q_max_len - q_len)\n","            q_new_len = q_len\n","        else:\n","            a_new_len = a_max_len\n","            q_new_len = q_max_len\n","            \n","            \n","        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n","            raise ValueError(\"New sequence length should be %d, but is %d\" \n","                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n","        \n","        t = t[:t_new_len]\n","        q = q[:q_new_len]\n","        a = a[:a_new_len]\n","    \n","    return t, q, a\n","\n","def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n","    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n","    \n","    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n","\n","    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n","    input_masks = _get_masks(stoken, max_sequence_length)\n","    input_segments = _get_segments(stoken, max_sequence_length)\n","\n","    return [input_ids, input_masks, input_segments]\n","\n","def compute_input_arays(df, columns, tokenizer, max_sequence_length):\n","    input_ids, input_masks, input_segments = [], [], []\n","    for _, instance in tqdm(df[columns].iterrows()):\n","        t, q, a = instance.question_title, instance.question_body, instance.answer\n","\n","        t, q, a = _trim_input(t, q, a, max_sequence_length)\n","\n","        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n","        input_ids.append(ids)\n","        input_masks.append(masks)\n","        input_segments.append(segments)\n","        \n","    return [np.asarray(input_ids, dtype=np.int32), \n","            np.asarray(input_masks, dtype=np.int32), \n","            np.asarray(input_segments, dtype=np.int32)]\n","\n","\n","def compute_output_arrays(df, columns):\n","    return np.asarray(df[columns])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zo-4yNUeXC7v","colab_type":"code","colab":{}},"source":["def compute_spearmanr(trues, preds):\n","    rhos = []\n","    for col_trues, col_pred in zip(trues.T, preds.T):\n","        rhos.append(\n","            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n","    return np.mean(rhos)\n","\n","\n","class CustomCallback(tf.keras.callbacks.Callback):\n","    \n","    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n","\n","        self.valid_inputs = valid_data[0]\n","        self.valid_outputs = valid_data[1]\n","        self.test_inputs = test_data\n","        \n","        self.batch_size = batch_size\n","        self.fold = fold\n","        \n","    def on_train_begin(self, logs={}):\n","        self.valid_predictions = []\n","        self.test_predictions = []\n","        \n","    def on_epoch_end(self, epoch, logs={}):\n","        self.valid_predictions.append(\n","            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n","        \n","        rho_val = compute_spearmanr(\n","            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n","        \n","        print(\"\\nvalidation rho: %.4f\" % rho_val)\n","        \n","        if self.fold is not None:\n","            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n","        \n","        self.test_predictions.append(\n","            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n","        )\n","\n","def bert_model():\n","    \n","    input_word_ids = tf.keras.layers.Input(\n","        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n","    input_masks = tf.keras.layers.Input(\n","        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n","    input_segments = tf.keras.layers.Input(\n","        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n","    \n","    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n","    \n","    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n","    \n","    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n","    x = tf.keras.layers.Dropout(0.2)(x)\n","    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n","\n","    model = tf.keras.models.Model(\n","        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n","    \n","    return model    \n","        \n","def train_and_predict(model, train_data, valid_data, test_data, \n","                      learning_rate, epochs, batch_size, loss_function, fold):\n","        \n","    custom_callback = CustomCallback(\n","        valid_data=(valid_data[0], valid_data[1]), \n","        test_data=test_data,\n","        batch_size=batch_size,\n","        fold=None)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(loss=loss_function, optimizer=optimizer)\n","    model.fit(train_data[0], train_data[1], epochs=epochs, \n","              batch_size=batch_size, callbacks=[custom_callback])\n","    \n","    return custom_callback"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7RrGHfJwXC7y","colab_type":"code","outputId":"83ea08df-86c4-45a2-8def-571585bb8738","executionInfo":{"status":"ok","timestamp":1578100321008,"user_tz":300,"elapsed":7295,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body) ############## originaln_splits=5\n","\n","#outputs = compute_output_arrays(df_train, output_categories)\n","#inputs = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","test_inputs = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"],"execution_count":63,"outputs":[{"output_type":"stream","text":["476it [00:02, 171.23it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"pnVTFX0yXC70","colab_type":"text"},"source":["## Check the performance of the model on each column"]},{"cell_type":"code","metadata":{"id":"qHEESBoEXC71","colab_type":"code","outputId":"8be80cd3-edd6-4b2d-f0e3-eb29e536bafe","executionInfo":{"status":"ok","timestamp":1578100323805,"user_tz":300,"elapsed":10046,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# Split dataset for validation from the train set\n","df_valid = df_train.sample(n=476)\n","df_valid_y = df_valid[df_valid.columns[11:]]\n","df_valid_x = df_valid[df_valid.columns[:11]]\n","\n","# Convert dataset to input\n","valid_inputs = compute_input_arays(df_valid_x, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"],"execution_count":64,"outputs":[{"output_type":"stream","text":["476it [00:02, 175.70it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-T5Pb-aaXC74","colab_type":"code","outputId":"993e4b4a-4386-4046-c297-e702cfea352a","executionInfo":{"status":"ok","timestamp":1578100323935,"user_tz":300,"elapsed":10137,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":536}},"source":["df_valid_x.head()"],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qa_id</th>\n","      <th>question_title</th>\n","      <th>question_body</th>\n","      <th>question_user_name</th>\n","      <th>question_user_page</th>\n","      <th>answer</th>\n","      <th>answer_user_name</th>\n","      <th>answer_user_page</th>\n","      <th>url</th>\n","      <th>category</th>\n","      <th>host</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5016</th>\n","      <td>7989</td>\n","      <td>Does tagging content affect SEO?</td>\n","      <td>Tagging content (i.e. categorizing content usi...</td>\n","      <td>Ken Liu</td>\n","      <td>https://webmasters.stackexchange.com/users/1837</td>\n","      <td>The tags won't affect the pages directly by be...</td>\n","      <td>John Conde</td>\n","      <td>https://webmasters.stackexchange.com/users/1253</td>\n","      <td>http://webmasters.stackexchange.com/questions/...</td>\n","      <td>TECHNOLOGY</td>\n","      <td>webmasters.stackexchange.com</td>\n","    </tr>\n","    <tr>\n","      <th>3762</th>\n","      <td>5989</td>\n","      <td>What is the best introductory Bayesian statist...</td>\n","      <td>Which is the best introductory textbook for Ba...</td>\n","      <td>Shane</td>\n","      <td>https://stats.stackexchange.com/users/5</td>\n","      <td>If I had to choose a single text for a beginne...</td>\n","      <td>Jim Stone</td>\n","      <td>https://stats.stackexchange.com/users/26779</td>\n","      <td>http://stats.stackexchange.com/questions/125/w...</td>\n","      <td>SCIENCE</td>\n","      <td>stats.stackexchange.com</td>\n","    </tr>\n","    <tr>\n","      <th>1691</th>\n","      <td>2675</td>\n","      <td>Is this a bug? Angular removes object params w...</td>\n","      <td>Please see this:\\n\\nhttp://plnkr.co/edit/soubK...</td>\n","      <td>user1804318</td>\n","      <td>https://stackoverflow.com/users/1804318</td>\n","      <td>Its not bug, its desired behaviourof angular, ...</td>\n","      <td>Milos Mosovsky</td>\n","      <td>https://stackoverflow.com/users/4530419</td>\n","      <td>http://stackoverflow.com/questions/31062344/is...</td>\n","      <td>STACKOVERFLOW</td>\n","      <td>stackoverflow.com</td>\n","    </tr>\n","    <tr>\n","      <th>3482</th>\n","      <td>5560</td>\n","      <td>How many creatures should be in a draft deck?</td>\n","      <td>How many creatures should the average, typical...</td>\n","      <td>JSBձոգչ</td>\n","      <td>https://boardgames.stackexchange.com/users/156</td>\n","      <td>Creatures are what makes you win and what make...</td>\n","      <td>Hackworth</td>\n","      <td>https://boardgames.stackexchange.com/users/1718</td>\n","      <td>http://boardgames.stackexchange.com/questions/...</td>\n","      <td>CULTURE</td>\n","      <td>boardgames.stackexchange.com</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>78</td>\n","      <td>Is potential energy and \"work done\" the same t...</td>\n","      <td>Is potential energy and \"work done\" the same t...</td>\n","      <td>Uzair</td>\n","      <td>https://physics.stackexchange.com/users/36559</td>\n","      <td>You will have to spent some energy to do work....</td>\n","      <td>Rajath Krishna R</td>\n","      <td>https://physics.stackexchange.com/users/23742</td>\n","      <td>http://physics.stackexchange.com/questions/940...</td>\n","      <td>SCIENCE</td>\n","      <td>physics.stackexchange.com</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      qa_id  ...                          host\n","5016   7989  ...  webmasters.stackexchange.com\n","3762   5989  ...       stats.stackexchange.com\n","1691   2675  ...             stackoverflow.com\n","3482   5560  ...  boardgames.stackexchange.com\n","51       78  ...     physics.stackexchange.com\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"GZYz54JrXC76","colab_type":"code","outputId":"a82a6b1b-b8e8-418c-a1cb-ad5f50751581","executionInfo":{"status":"ok","timestamp":1578100324030,"user_tz":300,"elapsed":10199,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":211}},"source":["df_valid_y.head()"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question_asker_intent_understanding</th>\n","      <th>question_body_critical</th>\n","      <th>question_conversational</th>\n","      <th>question_expect_short_answer</th>\n","      <th>question_fact_seeking</th>\n","      <th>question_has_commonly_accepted_answer</th>\n","      <th>question_interestingness_others</th>\n","      <th>question_interestingness_self</th>\n","      <th>question_multi_intent</th>\n","      <th>question_not_really_a_question</th>\n","      <th>question_opinion_seeking</th>\n","      <th>question_type_choice</th>\n","      <th>question_type_compare</th>\n","      <th>question_type_consequence</th>\n","      <th>question_type_definition</th>\n","      <th>question_type_entity</th>\n","      <th>question_type_instructions</th>\n","      <th>question_type_procedure</th>\n","      <th>question_type_reason_explanation</th>\n","      <th>question_type_spelling</th>\n","      <th>question_well_written</th>\n","      <th>answer_helpful</th>\n","      <th>answer_level_of_information</th>\n","      <th>answer_plausible</th>\n","      <th>answer_relevance</th>\n","      <th>answer_satisfaction</th>\n","      <th>answer_type_instructions</th>\n","      <th>answer_type_procedure</th>\n","      <th>answer_type_reason_explanation</th>\n","      <th>answer_well_written</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5016</th>\n","      <td>0.888889</td>\n","      <td>0.777778</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>0.555556</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.888889</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>0.888889</td>\n","      <td>0.933333</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>0.777778</td>\n","    </tr>\n","    <tr>\n","      <th>3762</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>0.555556</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.933333</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1691</th>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.555556</td>\n","      <td>0.555556</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.777778</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.933333</td>\n","      <td>0.0</td>\n","      <td>0.333333</td>\n","      <td>1.0</td>\n","      <td>0.555556</td>\n","    </tr>\n","    <tr>\n","      <th>3482</th>\n","      <td>0.666667</td>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.800000</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>1.000000</td>\n","      <td>0.888889</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.444444</td>\n","      <td>0.444444</td>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.333333</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.888889</td>\n","      <td>0.888889</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.933333</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>0.888889</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      question_asker_intent_understanding  ...  answer_well_written\n","5016                             0.888889  ...             0.777778\n","3762                             1.000000  ...             1.000000\n","1691                             1.000000  ...             0.555556\n","3482                             0.666667  ...             1.000000\n","51                               1.000000  ...             0.888889\n","\n","[5 rows x 30 columns]"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"4DjX20TrXC78","colab_type":"code","colab":{}},"source":["# Load trained models\n","#model_path = [f'bert/Full-0.h5', f'bert/bertuned_f0.h5', f'bert/bertuned_f1.h5', f'bert/bertuned_f2.h5', f'bert/bertuned_f3.h5', f'bert/bertuned_f4.h5']\n","model_path = [BERT_PATH+'bertuned_f0.h5', BERT_PATH+'bertuned_f1.h5', BERT_PATH+'bertuned_f2.h5',BERT_PATH+'bertuned_f3.h5',BERT_PATH+'bertuned_f4.h5']\n","models = []\n","\n","for i in range(len(model_path)):\n","    mp = model_path[i]\n","    model = bert_model()\n","    model.load_weights(mp)\n","    models.append(model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lWd6vwMbXC7-","colab_type":"code","outputId":"d078fdc7-d337-435e-f838-7412722369b0","executionInfo":{"status":"ok","timestamp":1578100487370,"user_tz":300,"elapsed":173507,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["len(models)"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"FSbKiTRaXC8B","colab_type":"code","outputId":"57a10f78-ee96-494b-d7b8-150059534d59","executionInfo":{"status":"ok","timestamp":1578100511699,"user_tz":300,"elapsed":197775,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# Predict the labels of validation set\n","valid_predictions = []\n","\n","for model in models[:1]:\n","    valid_predictions.append(model.predict(valid_inputs, batch_size=8, verbose=1))\n","\n","final_predictions_valid = np.mean(valid_predictions, axis=0)"],"execution_count":69,"outputs":[{"output_type":"stream","text":["476/476 [==============================] - 24s 51ms/sample\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mu6W6FYWXC8E","colab_type":"code","outputId":"60dc3a92-8007-4c11-9203-6cb475875581","executionInfo":{"status":"ok","timestamp":1578100511817,"user_tz":300,"elapsed":197882,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["df_valid_pred = pd.DataFrame(final_predictions_valid)\n","df_valid_pred"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.900936</td>\n","      <td>0.636685</td>\n","      <td>0.008015</td>\n","      <td>0.647269</td>\n","      <td>0.943153</td>\n","      <td>0.956474</td>\n","      <td>0.699755</td>\n","      <td>0.613185</td>\n","      <td>0.919710</td>\n","      <td>0.002984</td>\n","      <td>0.146274</td>\n","      <td>0.844482</td>\n","      <td>0.016048</td>\n","      <td>0.020196</td>\n","      <td>0.031633</td>\n","      <td>0.010348</td>\n","      <td>0.319826</td>\n","      <td>0.314135</td>\n","      <td>0.710218</td>\n","      <td>0.006181</td>\n","      <td>0.831946</td>\n","      <td>0.946026</td>\n","      <td>0.657676</td>\n","      <td>0.973172</td>\n","      <td>0.972096</td>\n","      <td>0.869471</td>\n","      <td>0.149882</td>\n","      <td>0.215696</td>\n","      <td>0.915390</td>\n","      <td>0.896545</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.946338</td>\n","      <td>0.821615</td>\n","      <td>0.116703</td>\n","      <td>0.841930</td>\n","      <td>0.606377</td>\n","      <td>0.165957</td>\n","      <td>0.743707</td>\n","      <td>0.537019</td>\n","      <td>0.060573</td>\n","      <td>0.006635</td>\n","      <td>0.568479</td>\n","      <td>0.187824</td>\n","      <td>0.087827</td>\n","      <td>0.012783</td>\n","      <td>0.012591</td>\n","      <td>0.716793</td>\n","      <td>0.015775</td>\n","      <td>0.012439</td>\n","      <td>0.044076</td>\n","      <td>0.003578</td>\n","      <td>0.910398</td>\n","      <td>0.937483</td>\n","      <td>0.810877</td>\n","      <td>0.936941</td>\n","      <td>0.963580</td>\n","      <td>0.886876</td>\n","      <td>0.083372</td>\n","      <td>0.020254</td>\n","      <td>0.202091</td>\n","      <td>0.948778</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.781372</td>\n","      <td>0.580100</td>\n","      <td>0.003585</td>\n","      <td>0.859610</td>\n","      <td>0.934892</td>\n","      <td>0.975744</td>\n","      <td>0.600435</td>\n","      <td>0.465962</td>\n","      <td>0.047681</td>\n","      <td>0.006678</td>\n","      <td>0.092950</td>\n","      <td>0.670140</td>\n","      <td>0.009381</td>\n","      <td>0.011022</td>\n","      <td>0.006443</td>\n","      <td>0.006961</td>\n","      <td>0.146667</td>\n","      <td>0.053523</td>\n","      <td>0.570909</td>\n","      <td>0.002577</td>\n","      <td>0.690440</td>\n","      <td>0.944655</td>\n","      <td>0.697969</td>\n","      <td>0.958595</td>\n","      <td>0.982449</td>\n","      <td>0.896239</td>\n","      <td>0.153790</td>\n","      <td>0.095089</td>\n","      <td>0.915511</td>\n","      <td>0.921333</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.869121</td>\n","      <td>0.489502</td>\n","      <td>0.017457</td>\n","      <td>0.466302</td>\n","      <td>0.720908</td>\n","      <td>0.552414</td>\n","      <td>0.634979</td>\n","      <td>0.503523</td>\n","      <td>0.924003</td>\n","      <td>0.002369</td>\n","      <td>0.727825</td>\n","      <td>0.101807</td>\n","      <td>0.047216</td>\n","      <td>0.010662</td>\n","      <td>0.015033</td>\n","      <td>0.163110</td>\n","      <td>0.818315</td>\n","      <td>0.097590</td>\n","      <td>0.141463</td>\n","      <td>0.001721</td>\n","      <td>0.801154</td>\n","      <td>0.961856</td>\n","      <td>0.681324</td>\n","      <td>0.976065</td>\n","      <td>0.970115</td>\n","      <td>0.840715</td>\n","      <td>0.863144</td>\n","      <td>0.115209</td>\n","      <td>0.409733</td>\n","      <td>0.939759</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.932840</td>\n","      <td>0.804420</td>\n","      <td>0.005942</td>\n","      <td>0.749478</td>\n","      <td>0.972538</td>\n","      <td>0.954982</td>\n","      <td>0.728009</td>\n","      <td>0.633627</td>\n","      <td>0.531264</td>\n","      <td>0.001939</td>\n","      <td>0.060953</td>\n","      <td>0.897610</td>\n","      <td>0.068490</td>\n","      <td>0.015496</td>\n","      <td>0.267029</td>\n","      <td>0.008875</td>\n","      <td>0.003056</td>\n","      <td>0.006596</td>\n","      <td>0.790419</td>\n","      <td>0.004825</td>\n","      <td>0.863293</td>\n","      <td>0.952494</td>\n","      <td>0.596452</td>\n","      <td>0.979766</td>\n","      <td>0.985870</td>\n","      <td>0.842139</td>\n","      <td>0.005526</td>\n","      <td>0.006889</td>\n","      <td>0.966770</td>\n","      <td>0.919844</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>471</th>\n","      <td>0.751196</td>\n","      <td>0.353556</td>\n","      <td>0.004007</td>\n","      <td>0.594660</td>\n","      <td>0.886813</td>\n","      <td>0.945158</td>\n","      <td>0.643245</td>\n","      <td>0.435616</td>\n","      <td>0.064067</td>\n","      <td>0.010862</td>\n","      <td>0.207092</td>\n","      <td>0.016222</td>\n","      <td>0.004638</td>\n","      <td>0.005037</td>\n","      <td>0.009631</td>\n","      <td>0.018084</td>\n","      <td>0.563622</td>\n","      <td>0.111902</td>\n","      <td>0.606537</td>\n","      <td>0.002674</td>\n","      <td>0.661752</td>\n","      <td>0.898347</td>\n","      <td>0.670846</td>\n","      <td>0.957778</td>\n","      <td>0.959994</td>\n","      <td>0.844200</td>\n","      <td>0.499427</td>\n","      <td>0.151637</td>\n","      <td>0.823521</td>\n","      <td>0.878024</td>\n","    </tr>\n","    <tr>\n","      <th>472</th>\n","      <td>0.901698</td>\n","      <td>0.548917</td>\n","      <td>0.002574</td>\n","      <td>0.702136</td>\n","      <td>0.905775</td>\n","      <td>0.976769</td>\n","      <td>0.594572</td>\n","      <td>0.542414</td>\n","      <td>0.104331</td>\n","      <td>0.005444</td>\n","      <td>0.210963</td>\n","      <td>0.020617</td>\n","      <td>0.003278</td>\n","      <td>0.006408</td>\n","      <td>0.003644</td>\n","      <td>0.003062</td>\n","      <td>0.828739</td>\n","      <td>0.149893</td>\n","      <td>0.383005</td>\n","      <td>0.001216</td>\n","      <td>0.759161</td>\n","      <td>0.960411</td>\n","      <td>0.723583</td>\n","      <td>0.985729</td>\n","      <td>0.984110</td>\n","      <td>0.899519</td>\n","      <td>0.683371</td>\n","      <td>0.212474</td>\n","      <td>0.916331</td>\n","      <td>0.922766</td>\n","    </tr>\n","    <tr>\n","      <th>473</th>\n","      <td>0.828628</td>\n","      <td>0.344384</td>\n","      <td>0.134545</td>\n","      <td>0.246080</td>\n","      <td>0.556089</td>\n","      <td>0.170132</td>\n","      <td>0.686091</td>\n","      <td>0.525992</td>\n","      <td>0.894091</td>\n","      <td>0.009399</td>\n","      <td>0.842937</td>\n","      <td>0.462820</td>\n","      <td>0.695428</td>\n","      <td>0.018750</td>\n","      <td>0.032859</td>\n","      <td>0.032793</td>\n","      <td>0.368557</td>\n","      <td>0.065566</td>\n","      <td>0.405686</td>\n","      <td>0.005681</td>\n","      <td>0.815821</td>\n","      <td>0.920210</td>\n","      <td>0.688353</td>\n","      <td>0.971101</td>\n","      <td>0.946241</td>\n","      <td>0.810855</td>\n","      <td>0.335266</td>\n","      <td>0.050472</td>\n","      <td>0.508325</td>\n","      <td>0.909593</td>\n","    </tr>\n","    <tr>\n","      <th>474</th>\n","      <td>0.764244</td>\n","      <td>0.693027</td>\n","      <td>0.007079</td>\n","      <td>0.760026</td>\n","      <td>0.890182</td>\n","      <td>0.952749</td>\n","      <td>0.537249</td>\n","      <td>0.389737</td>\n","      <td>0.242600</td>\n","      <td>0.003701</td>\n","      <td>0.205656</td>\n","      <td>0.960925</td>\n","      <td>0.014490</td>\n","      <td>0.017041</td>\n","      <td>0.004650</td>\n","      <td>0.005269</td>\n","      <td>0.155606</td>\n","      <td>0.039231</td>\n","      <td>0.700660</td>\n","      <td>0.001678</td>\n","      <td>0.795438</td>\n","      <td>0.969791</td>\n","      <td>0.661764</td>\n","      <td>0.978213</td>\n","      <td>0.985048</td>\n","      <td>0.929930</td>\n","      <td>0.584588</td>\n","      <td>0.116188</td>\n","      <td>0.882187</td>\n","      <td>0.935381</td>\n","    </tr>\n","    <tr>\n","      <th>475</th>\n","      <td>0.895291</td>\n","      <td>0.453698</td>\n","      <td>0.007055</td>\n","      <td>0.675214</td>\n","      <td>0.560796</td>\n","      <td>0.875970</td>\n","      <td>0.622983</td>\n","      <td>0.537028</td>\n","      <td>0.063180</td>\n","      <td>0.004720</td>\n","      <td>0.666860</td>\n","      <td>0.060739</td>\n","      <td>0.007320</td>\n","      <td>0.002607</td>\n","      <td>0.001900</td>\n","      <td>0.003069</td>\n","      <td>0.951911</td>\n","      <td>0.076906</td>\n","      <td>0.082745</td>\n","      <td>0.000888</td>\n","      <td>0.707347</td>\n","      <td>0.974178</td>\n","      <td>0.682593</td>\n","      <td>0.989571</td>\n","      <td>0.989517</td>\n","      <td>0.926400</td>\n","      <td>0.967420</td>\n","      <td>0.119011</td>\n","      <td>0.530201</td>\n","      <td>0.944515</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>476 rows × 30 columns</p>\n","</div>"],"text/plain":["           0         1         2   ...        27        28        29\n","0    0.900936  0.636685  0.008015  ...  0.215696  0.915390  0.896545\n","1    0.946338  0.821615  0.116703  ...  0.020254  0.202091  0.948778\n","2    0.781372  0.580100  0.003585  ...  0.095089  0.915511  0.921333\n","3    0.869121  0.489502  0.017457  ...  0.115209  0.409733  0.939759\n","4    0.932840  0.804420  0.005942  ...  0.006889  0.966770  0.919844\n","..        ...       ...       ...  ...       ...       ...       ...\n","471  0.751196  0.353556  0.004007  ...  0.151637  0.823521  0.878024\n","472  0.901698  0.548917  0.002574  ...  0.212474  0.916331  0.922766\n","473  0.828628  0.344384  0.134545  ...  0.050472  0.508325  0.909593\n","474  0.764244  0.693027  0.007079  ...  0.116188  0.882187  0.935381\n","475  0.895291  0.453698  0.007055  ...  0.119011  0.530201  0.944515\n","\n","[476 rows x 30 columns]"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"uvJpOcHfXC8G","colab_type":"code","outputId":"f0300d35-f17e-4f56-96cd-8028d0f1e52e","executionInfo":{"status":"ok","timestamp":1578100511818,"user_tz":300,"elapsed":197873,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["df_valid_y"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question_asker_intent_understanding</th>\n","      <th>question_body_critical</th>\n","      <th>question_conversational</th>\n","      <th>question_expect_short_answer</th>\n","      <th>question_fact_seeking</th>\n","      <th>question_has_commonly_accepted_answer</th>\n","      <th>question_interestingness_others</th>\n","      <th>question_interestingness_self</th>\n","      <th>question_multi_intent</th>\n","      <th>question_not_really_a_question</th>\n","      <th>question_opinion_seeking</th>\n","      <th>question_type_choice</th>\n","      <th>question_type_compare</th>\n","      <th>question_type_consequence</th>\n","      <th>question_type_definition</th>\n","      <th>question_type_entity</th>\n","      <th>question_type_instructions</th>\n","      <th>question_type_procedure</th>\n","      <th>question_type_reason_explanation</th>\n","      <th>question_type_spelling</th>\n","      <th>question_well_written</th>\n","      <th>answer_helpful</th>\n","      <th>answer_level_of_information</th>\n","      <th>answer_plausible</th>\n","      <th>answer_relevance</th>\n","      <th>answer_satisfaction</th>\n","      <th>answer_type_instructions</th>\n","      <th>answer_type_procedure</th>\n","      <th>answer_type_reason_explanation</th>\n","      <th>answer_well_written</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5016</th>\n","      <td>0.888889</td>\n","      <td>0.777778</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>0.555556</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.888889</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>0.888889</td>\n","      <td>0.933333</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.777778</td>\n","    </tr>\n","    <tr>\n","      <th>3762</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>0.555556</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.933333</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1691</th>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.555556</td>\n","      <td>0.555556</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.777778</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.933333</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>1.000000</td>\n","      <td>0.555556</td>\n","    </tr>\n","    <tr>\n","      <th>3482</th>\n","      <td>0.666667</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.800000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>1.000000</td>\n","      <td>0.888889</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.444444</td>\n","      <td>0.444444</td>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.333333</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.888889</td>\n","      <td>0.888889</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.933333</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.888889</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4892</th>\n","      <td>0.777778</td>\n","      <td>0.555556</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>0.444444</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.333333</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.777778</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.933333</td>\n","      <td>0.666667</td>\n","      <td>0.333333</td>\n","      <td>1.000000</td>\n","      <td>0.888889</td>\n","    </tr>\n","    <tr>\n","      <th>719</th>\n","      <td>0.888889</td>\n","      <td>0.444444</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.555556</td>\n","      <td>0.444444</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.888889</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.933333</td>\n","      <td>0.666667</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>0.888889</td>\n","    </tr>\n","    <tr>\n","      <th>5940</th>\n","      <td>0.888889</td>\n","      <td>0.333333</td>\n","      <td>0.666667</td>\n","      <td>0.333333</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.555556</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>0.333333</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.888889</td>\n","      <td>0.888889</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>0.888889</td>\n","      <td>0.866667</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1992</th>\n","      <td>0.777778</td>\n","      <td>0.777778</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>0.333333</td>\n","      <td>0.333333</td>\n","      <td>0.333333</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>0.0</td>\n","      <td>0.777778</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3170</th>\n","      <td>0.888889</td>\n","      <td>0.555556</td>\n","      <td>0.000000</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.666667</td>\n","      <td>0.555556</td>\n","      <td>0.444444</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.666667</td>\n","      <td>1.000000</td>\n","      <td>0.888889</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>0.933333</td>\n","      <td>1.000000</td>\n","      <td>0.333333</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>476 rows × 30 columns</p>\n","</div>"],"text/plain":["      question_asker_intent_understanding  ...  answer_well_written\n","5016                             0.888889  ...             0.777778\n","3762                             1.000000  ...             1.000000\n","1691                             1.000000  ...             0.555556\n","3482                             0.666667  ...             1.000000\n","51                               1.000000  ...             0.888889\n","...                                   ...  ...                  ...\n","4892                             0.777778  ...             0.888889\n","719                              0.888889  ...             0.888889\n","5940                             0.888889  ...             1.000000\n","1992                             0.777778  ...             1.000000\n","3170                             0.888889  ...             1.000000\n","\n","[476 rows x 30 columns]"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"E2pWqk0CXC8I","colab_type":"code","outputId":"f4e41b87-7e4e-4496-fdd2-ca62f4500762","executionInfo":{"status":"ok","timestamp":1578100511949,"user_tz":300,"elapsed":197984,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":487}},"source":["df_test.head()"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qa_id</th>\n","      <th>question_title</th>\n","      <th>question_body</th>\n","      <th>question_user_name</th>\n","      <th>question_user_page</th>\n","      <th>answer</th>\n","      <th>answer_user_name</th>\n","      <th>answer_user_page</th>\n","      <th>url</th>\n","      <th>category</th>\n","      <th>host</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39</td>\n","      <td>Will leaving corpses lying around upset my pri...</td>\n","      <td>I see questions/information online about how t...</td>\n","      <td>Dylan</td>\n","      <td>https://gaming.stackexchange.com/users/64471</td>\n","      <td>There is no consequence for leaving corpses an...</td>\n","      <td>Nelson868</td>\n","      <td>https://gaming.stackexchange.com/users/97324</td>\n","      <td>http://gaming.stackexchange.com/questions/1979...</td>\n","      <td>CULTURE</td>\n","      <td>gaming.stackexchange.com</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>46</td>\n","      <td>Url link to feature image in the portfolio</td>\n","      <td>I am new to Wordpress. i have issue with Featu...</td>\n","      <td>Anu</td>\n","      <td>https://wordpress.stackexchange.com/users/72927</td>\n","      <td>I think it is possible with custom fields.\\n\\n...</td>\n","      <td>Irina</td>\n","      <td>https://wordpress.stackexchange.com/users/27233</td>\n","      <td>http://wordpress.stackexchange.com/questions/1...</td>\n","      <td>TECHNOLOGY</td>\n","      <td>wordpress.stackexchange.com</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>70</td>\n","      <td>Is accuracy, recoil or bullet spread affected ...</td>\n","      <td>To experiment I started a bot game, toggled in...</td>\n","      <td>Konsta</td>\n","      <td>https://gaming.stackexchange.com/users/37545</td>\n","      <td>You do not have armour in the screenshots. Thi...</td>\n","      <td>Damon Smithies</td>\n","      <td>https://gaming.stackexchange.com/users/70641</td>\n","      <td>http://gaming.stackexchange.com/questions/2154...</td>\n","      <td>CULTURE</td>\n","      <td>gaming.stackexchange.com</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>132</td>\n","      <td>Suddenly got an I/O error from my external HDD</td>\n","      <td>I have used my Raspberry Pi as a torrent-serve...</td>\n","      <td>robbannn</td>\n","      <td>https://raspberrypi.stackexchange.com/users/17341</td>\n","      <td>Your Western Digital hard drive is disappearin...</td>\n","      <td>HeatfanJohn</td>\n","      <td>https://raspberrypi.stackexchange.com/users/1311</td>\n","      <td>http://raspberrypi.stackexchange.com/questions...</td>\n","      <td>TECHNOLOGY</td>\n","      <td>raspberrypi.stackexchange.com</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>200</td>\n","      <td>Passenger Name - Flight Booking Passenger only...</td>\n","      <td>I have bought Delhi-London return flights for ...</td>\n","      <td>Amit</td>\n","      <td>https://travel.stackexchange.com/users/29089</td>\n","      <td>I called two persons who work for Saudia (tick...</td>\n","      <td>Nean Der Thal</td>\n","      <td>https://travel.stackexchange.com/users/10051</td>\n","      <td>http://travel.stackexchange.com/questions/4704...</td>\n","      <td>CULTURE</td>\n","      <td>travel.stackexchange.com</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   qa_id  ...                           host\n","0     39  ...       gaming.stackexchange.com\n","1     46  ...    wordpress.stackexchange.com\n","2     70  ...       gaming.stackexchange.com\n","3    132  ...  raspberrypi.stackexchange.com\n","4    200  ...       travel.stackexchange.com\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"Ch7fgwVeXC8K","colab_type":"text"},"source":["### Check the scores using spearmanr from scipy"]},{"cell_type":"code","metadata":{"id":"pEj-EP-oXC8M","colab_type":"code","outputId":"77a8f331-3e41-4c0d-82a2-54c02a20bc4d","executionInfo":{"status":"ok","timestamp":1578100511949,"user_tz":300,"elapsed":197955,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# performance of each column in the validation dataset, model 1\n","from scipy.stats import spearmanr\n","\n","df_spearman = pd.DataFrame(df_valid_y.columns,columns=[\"Label\"])\n","coefs = []\n","\n","for i in range(30):\n","    coef,p = spearmanr(df_valid_pred.iloc[:,i], df_valid_y.iloc[:,i])\n","    coefs.append(coef)\n","\n","df_spearman[\"Score\"] = coefs\n","df_spearman"],"execution_count":73,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n","  c /= stddev[:, None]\n","/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n","  c /= stddev[None, :]\n","/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n","  return (a < x) & (x < b)\n","/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n","  return (a < x) & (x < b)\n","/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n","  cond2 = cond0 & (x <= _a)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>question_asker_intent_understanding</td>\n","      <td>0.555494</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>question_body_critical</td>\n","      <td>0.755898</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>question_conversational</td>\n","      <td>0.471938</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>question_expect_short_answer</td>\n","      <td>0.591397</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>question_fact_seeking</td>\n","      <td>0.644531</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>question_has_commonly_accepted_answer</td>\n","      <td>0.624263</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>question_interestingness_others</td>\n","      <td>0.487013</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>question_interestingness_self</td>\n","      <td>0.605696</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>question_multi_intent</td>\n","      <td>0.763799</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>question_not_really_a_question</td>\n","      <td>0.122518</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>question_opinion_seeking</td>\n","      <td>0.754730</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>question_type_choice</td>\n","      <td>0.857066</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>question_type_compare</td>\n","      <td>0.356107</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>question_type_consequence</td>\n","      <td>0.261401</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>question_type_definition</td>\n","      <td>0.382149</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>question_type_entity</td>\n","      <td>0.525815</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>question_type_instructions</td>\n","      <td>0.873799</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>question_type_procedure</td>\n","      <td>0.615008</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>question_type_reason_explanation</td>\n","      <td>0.812235</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>question_type_spelling</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>question_well_written</td>\n","      <td>0.658766</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>answer_helpful</td>\n","      <td>0.431245</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>answer_level_of_information</td>\n","      <td>0.480258</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>answer_plausible</td>\n","      <td>0.319976</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>answer_relevance</td>\n","      <td>0.356440</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>answer_satisfaction</td>\n","      <td>0.497878</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>answer_type_instructions</td>\n","      <td>0.873602</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>answer_type_procedure</td>\n","      <td>0.536027</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>answer_type_reason_explanation</td>\n","      <td>0.843640</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>answer_well_written</td>\n","      <td>0.335844</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                    Label     Score\n","0     question_asker_intent_understanding  0.555494\n","1                  question_body_critical  0.755898\n","2                 question_conversational  0.471938\n","3            question_expect_short_answer  0.591397\n","4                   question_fact_seeking  0.644531\n","5   question_has_commonly_accepted_answer  0.624263\n","6         question_interestingness_others  0.487013\n","7           question_interestingness_self  0.605696\n","8                   question_multi_intent  0.763799\n","9          question_not_really_a_question  0.122518\n","10               question_opinion_seeking  0.754730\n","11                   question_type_choice  0.857066\n","12                  question_type_compare  0.356107\n","13              question_type_consequence  0.261401\n","14               question_type_definition  0.382149\n","15                   question_type_entity  0.525815\n","16             question_type_instructions  0.873799\n","17                question_type_procedure  0.615008\n","18       question_type_reason_explanation  0.812235\n","19                 question_type_spelling       NaN\n","20                  question_well_written  0.658766\n","21                         answer_helpful  0.431245\n","22            answer_level_of_information  0.480258\n","23                       answer_plausible  0.319976\n","24                       answer_relevance  0.356440\n","25                    answer_satisfaction  0.497878\n","26               answer_type_instructions  0.873602\n","27                  answer_type_procedure  0.536027\n","28         answer_type_reason_explanation  0.843640\n","29                    answer_well_written  0.335844"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"BCCpkzlLGfZq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"d99b3391-c955-4b98-ef20-6f194949a9c8","executionInfo":{"status":"ok","timestamp":1578100512088,"user_tz":300,"elapsed":198078,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}}},"source":["df_spearman.mean()"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Score    0.565329\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"markdown","metadata":{"id":"xnGfse8eXC8O","colab_type":"text"},"source":["The scores of question_not_really_a_question(Label 10, 0.157999), question_type_consequence(Label 14, 0.187766) and question_type_spelling(Label 20, NaN) is significantly lower than others."]},{"cell_type":"markdown","metadata":{"id":"__Nk-v2oXC8P","colab_type":"text"},"source":["After checking the train set, we find the data of these columns is imbalanced. So, we're considering solving this problem using imblearn or other methods."]},{"cell_type":"markdown","metadata":{"id":"QgenhN4Fb0RW","colab_type":"text"},"source":["### Try Multilabel Stratified KFold to tackle umbalanced data"]},{"cell_type":"code","metadata":{"id":"fXmxekLzeCHY","colab_type":"code","outputId":"479fbcf1-9a8c-4a89-a11a-e2a75c3033bd","executionInfo":{"status":"ok","timestamp":1578100515081,"user_tz":300,"elapsed":201061,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":98}},"source":["# !pip install six\n","!pip install iterative-stratification"],"execution_count":75,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.6/dist-packages (0.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.17.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.3.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.21.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.14.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fb6vlxDnbzkV","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n","SEED = 42\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sNRFveZexIU","colab_type":"code","colab":{}},"source":["def bagged_model(valid_inputs):\n","  valid_predictions = []\n","  for model in models[:1]:\n","    valid_predictions.append(model.predict(valid_inputs, batch_size=8, verbose=1))\n","  final_predictions_valid = np.nanmean(valid_predictions, axis=0)\n","  return final_predictions_valid"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vM40rzMeqYgw","colab_type":"code","colab":{}},"source":["def hstack_input_arays(df, columns, tokenizer, max_sequence_length):\n","    input_ids, input_masks, input_segments = [], [], []\n","    for _, instance in tqdm(df[columns].iterrows()):\n","        t, q, a = instance.question_title, instance.question_body, instance.answer\n","\n","        t, q, a = _trim_input(t, q, a, max_sequence_length)\n","\n","        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n","        input_ids.append(ids)\n","        input_masks.append(masks)\n","        input_segments.append(segments)\n","        \n","    return np.hstack([np.asarray(input_ids, dtype=np.int32)]+ \n","            [np.asarray(input_masks, dtype=np.int32)]+\n","            [np.asarray(input_segments, dtype=np.int32)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQ7zht9Ii92r","colab_type":"code","outputId":"95e21b78-4a58-4423-aed1-6e166734ac16","executionInfo":{"status":"ok","timestamp":1578100733091,"user_tz":300,"elapsed":419029,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["NUM_FOLDS=4\n","kf = MultilabelStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n","\n","\n","y_train = df_train[df_train.columns[11:]].values\n","X_train = df_train[df_train.columns[:11]]\n","\n","X_train2 = hstack_input_arays(X_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","\n","df_spearman_stkfold = pd.DataFrame(df_train.columns[11:],columns=[\"Label\"])\n","\n","for ind, (tr, val) in enumerate(kf.split(X_train2, y_train)):\n","    #X_tr = X_train[tr]\n","    #y_tr = y_train[tr]\n","    X_vl = X_train.loc[val]\n","    y_vl = y_train[val]\n","\n","    x_input = compute_input_arays(X_vl,input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","\n","    df_valid_pred = bagged_model(x_input)\n","    \n","    coefs = []\n","\n","    for i in range(30):\n","      coef,p = spearmanr(df_valid_pred[:,i], y_vl[:,i])\n","      coefs.append(coef)\n","\n","    colname = 'Score'+str(ind)\n","    df_spearman_stkfold[colname] = coefs\n","           "],"execution_count":79,"outputs":[{"output_type":"stream","text":["6079it [00:34, 176.17it/s]\n","1522it [00:08, 176.59it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["1522/1522 [==============================] - 37s 24ms/sample\n"],"name":"stdout"},{"output_type":"stream","text":["1509it [00:08, 181.28it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["1509/1509 [==============================] - 37s 24ms/sample\n"],"name":"stdout"},{"output_type":"stream","text":["1535it [00:08, 178.36it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["1535/1535 [==============================] - 37s 24ms/sample\n"],"name":"stdout"},{"output_type":"stream","text":["1513it [00:08, 180.27it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["1513/1513 [==============================] - 37s 24ms/sample\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dxKG00deoGKH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":910},"outputId":"92fe2847-2781-4a8d-8ed6-e6522f7b7fde","executionInfo":{"status":"ok","timestamp":1578100733177,"user_tz":300,"elapsed":419106,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}}},"source":["df_spearman_stkfold"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Score0</th>\n","      <th>Score1</th>\n","      <th>Score2</th>\n","      <th>Score3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>question_asker_intent_understanding</td>\n","      <td>0.533462</td>\n","      <td>0.524203</td>\n","      <td>0.542171</td>\n","      <td>0.509370</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>question_body_critical</td>\n","      <td>0.747058</td>\n","      <td>0.752342</td>\n","      <td>0.726799</td>\n","      <td>0.722786</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>question_conversational</td>\n","      <td>0.478382</td>\n","      <td>0.463704</td>\n","      <td>0.463265</td>\n","      <td>0.444813</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>question_expect_short_answer</td>\n","      <td>0.596599</td>\n","      <td>0.650699</td>\n","      <td>0.600397</td>\n","      <td>0.586654</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>question_fact_seeking</td>\n","      <td>0.672332</td>\n","      <td>0.681383</td>\n","      <td>0.678991</td>\n","      <td>0.647743</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>question_has_commonly_accepted_answer</td>\n","      <td>0.652439</td>\n","      <td>0.632285</td>\n","      <td>0.604023</td>\n","      <td>0.646079</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>question_interestingness_others</td>\n","      <td>0.471137</td>\n","      <td>0.492640</td>\n","      <td>0.495568</td>\n","      <td>0.474275</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>question_interestingness_self</td>\n","      <td>0.626630</td>\n","      <td>0.645253</td>\n","      <td>0.637935</td>\n","      <td>0.627667</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>question_multi_intent</td>\n","      <td>0.720586</td>\n","      <td>0.725564</td>\n","      <td>0.713097</td>\n","      <td>0.732977</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>question_not_really_a_question</td>\n","      <td>0.147880</td>\n","      <td>0.128152</td>\n","      <td>0.121515</td>\n","      <td>0.121788</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>question_opinion_seeking</td>\n","      <td>0.769854</td>\n","      <td>0.762916</td>\n","      <td>0.755429</td>\n","      <td>0.762302</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>question_type_choice</td>\n","      <td>0.820896</td>\n","      <td>0.831297</td>\n","      <td>0.834324</td>\n","      <td>0.838395</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>question_type_compare</td>\n","      <td>0.404264</td>\n","      <td>0.391192</td>\n","      <td>0.404175</td>\n","      <td>0.402861</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>question_type_consequence</td>\n","      <td>0.198246</td>\n","      <td>0.192789</td>\n","      <td>0.210448</td>\n","      <td>0.222155</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>question_type_definition</td>\n","      <td>0.385421</td>\n","      <td>0.377895</td>\n","      <td>0.386460</td>\n","      <td>0.363571</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>question_type_entity</td>\n","      <td>0.515788</td>\n","      <td>0.517655</td>\n","      <td>0.503752</td>\n","      <td>0.514561</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>question_type_instructions</td>\n","      <td>0.876886</td>\n","      <td>0.876304</td>\n","      <td>0.870343</td>\n","      <td>0.866719</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>question_type_procedure</td>\n","      <td>0.603116</td>\n","      <td>0.601391</td>\n","      <td>0.619146</td>\n","      <td>0.628992</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>question_type_reason_explanation</td>\n","      <td>0.797517</td>\n","      <td>0.809719</td>\n","      <td>0.826345</td>\n","      <td>0.834545</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>question_type_spelling</td>\n","      <td>0.076822</td>\n","      <td>0.077015</td>\n","      <td>0.076297</td>\n","      <td>0.042933</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>question_well_written</td>\n","      <td>0.672413</td>\n","      <td>0.656457</td>\n","      <td>0.686576</td>\n","      <td>0.661415</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>answer_helpful</td>\n","      <td>0.458419</td>\n","      <td>0.436684</td>\n","      <td>0.446632</td>\n","      <td>0.395510</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>answer_level_of_information</td>\n","      <td>0.522179</td>\n","      <td>0.473064</td>\n","      <td>0.525054</td>\n","      <td>0.501170</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>answer_plausible</td>\n","      <td>0.360943</td>\n","      <td>0.364001</td>\n","      <td>0.339336</td>\n","      <td>0.326504</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>answer_relevance</td>\n","      <td>0.350830</td>\n","      <td>0.339401</td>\n","      <td>0.343496</td>\n","      <td>0.317568</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>answer_satisfaction</td>\n","      <td>0.486494</td>\n","      <td>0.498130</td>\n","      <td>0.513673</td>\n","      <td>0.493645</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>answer_type_instructions</td>\n","      <td>0.868607</td>\n","      <td>0.881088</td>\n","      <td>0.859688</td>\n","      <td>0.873337</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>answer_type_procedure</td>\n","      <td>0.538484</td>\n","      <td>0.549984</td>\n","      <td>0.538683</td>\n","      <td>0.533560</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>answer_type_reason_explanation</td>\n","      <td>0.852565</td>\n","      <td>0.864154</td>\n","      <td>0.863586</td>\n","      <td>0.845633</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>answer_well_written</td>\n","      <td>0.348771</td>\n","      <td>0.390557</td>\n","      <td>0.357891</td>\n","      <td>0.320558</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                    Label    Score0  ...    Score2    Score3\n","0     question_asker_intent_understanding  0.533462  ...  0.542171  0.509370\n","1                  question_body_critical  0.747058  ...  0.726799  0.722786\n","2                 question_conversational  0.478382  ...  0.463265  0.444813\n","3            question_expect_short_answer  0.596599  ...  0.600397  0.586654\n","4                   question_fact_seeking  0.672332  ...  0.678991  0.647743\n","5   question_has_commonly_accepted_answer  0.652439  ...  0.604023  0.646079\n","6         question_interestingness_others  0.471137  ...  0.495568  0.474275\n","7           question_interestingness_self  0.626630  ...  0.637935  0.627667\n","8                   question_multi_intent  0.720586  ...  0.713097  0.732977\n","9          question_not_really_a_question  0.147880  ...  0.121515  0.121788\n","10               question_opinion_seeking  0.769854  ...  0.755429  0.762302\n","11                   question_type_choice  0.820896  ...  0.834324  0.838395\n","12                  question_type_compare  0.404264  ...  0.404175  0.402861\n","13              question_type_consequence  0.198246  ...  0.210448  0.222155\n","14               question_type_definition  0.385421  ...  0.386460  0.363571\n","15                   question_type_entity  0.515788  ...  0.503752  0.514561\n","16             question_type_instructions  0.876886  ...  0.870343  0.866719\n","17                question_type_procedure  0.603116  ...  0.619146  0.628992\n","18       question_type_reason_explanation  0.797517  ...  0.826345  0.834545\n","19                 question_type_spelling  0.076822  ...  0.076297  0.042933\n","20                  question_well_written  0.672413  ...  0.686576  0.661415\n","21                         answer_helpful  0.458419  ...  0.446632  0.395510\n","22            answer_level_of_information  0.522179  ...  0.525054  0.501170\n","23                       answer_plausible  0.360943  ...  0.339336  0.326504\n","24                       answer_relevance  0.350830  ...  0.343496  0.317568\n","25                    answer_satisfaction  0.486494  ...  0.513673  0.493645\n","26               answer_type_instructions  0.868607  ...  0.859688  0.873337\n","27                  answer_type_procedure  0.538484  ...  0.538683  0.533560\n","28         answer_type_reason_explanation  0.852565  ...  0.863586  0.845633\n","29                    answer_well_written  0.348771  ...  0.357891  0.320558\n","\n","[30 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"4cQx6PiBAUvn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"f6f75a05-89d9-4c33-bc5d-239e20a7c3ef","executionInfo":{"status":"ok","timestamp":1578100733178,"user_tz":300,"elapsed":419097,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}}},"source":["df_spearman_stkfold.mean().mean()"],"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5495676537054117"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"markdown","metadata":{"id":"YM403hDNGXRu","colab_type":"text"},"source":["Using Stratified KFold improved our Spearman scores from "]},{"cell_type":"markdown","metadata":{"id":"LAZAf5i8XC8Q","colab_type":"text"},"source":["## Run the model on the test set"]},{"cell_type":"code","metadata":{"id":"DTJlz5LfXC8b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"e86d8bdc-488e-48c7-8e9f-9099de469b09","executionInfo":{"status":"ok","timestamp":1578100744714,"user_tz":300,"elapsed":430624,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}}},"source":["final_predictions = bagged_model(test_inputs)"],"execution_count":82,"outputs":[{"output_type":"stream","text":["476/476 [==============================] - 12s 24ms/sample\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GCdJPnIHXC8f","colab_type":"code","colab":{}},"source":["df_sub.iloc[:, 1:] = final_predictions\n","df_sub.to_csv('submission.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oNyBnNocXC8j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"cb8d1a36-75c7-46ef-c978-38c43f37aba3","executionInfo":{"status":"ok","timestamp":1578100744786,"user_tz":300,"elapsed":430674,"user":{"displayName":"Emmy Phung","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDgmhNyJiOVBE-yI9IiXTWzyNeqhzNGDyMfrC8c=s64","userId":"06403056978504730323"}}},"source":["df_sub.head()"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qa_id</th>\n","      <th>question_asker_intent_understanding</th>\n","      <th>question_body_critical</th>\n","      <th>question_conversational</th>\n","      <th>question_expect_short_answer</th>\n","      <th>question_fact_seeking</th>\n","      <th>question_has_commonly_accepted_answer</th>\n","      <th>question_interestingness_others</th>\n","      <th>question_interestingness_self</th>\n","      <th>question_multi_intent</th>\n","      <th>question_not_really_a_question</th>\n","      <th>question_opinion_seeking</th>\n","      <th>question_type_choice</th>\n","      <th>question_type_compare</th>\n","      <th>question_type_consequence</th>\n","      <th>question_type_definition</th>\n","      <th>question_type_entity</th>\n","      <th>question_type_instructions</th>\n","      <th>question_type_procedure</th>\n","      <th>question_type_reason_explanation</th>\n","      <th>question_type_spelling</th>\n","      <th>question_well_written</th>\n","      <th>answer_helpful</th>\n","      <th>answer_level_of_information</th>\n","      <th>answer_plausible</th>\n","      <th>answer_relevance</th>\n","      <th>answer_satisfaction</th>\n","      <th>answer_type_instructions</th>\n","      <th>answer_type_procedure</th>\n","      <th>answer_type_reason_explanation</th>\n","      <th>answer_well_written</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39</td>\n","      <td>0.922638</td>\n","      <td>0.567927</td>\n","      <td>0.064251</td>\n","      <td>0.608264</td>\n","      <td>0.711082</td>\n","      <td>0.850872</td>\n","      <td>0.713220</td>\n","      <td>0.661823</td>\n","      <td>0.860829</td>\n","      <td>0.003948</td>\n","      <td>0.644606</td>\n","      <td>0.834519</td>\n","      <td>0.008475</td>\n","      <td>0.275376</td>\n","      <td>0.010538</td>\n","      <td>0.011853</td>\n","      <td>0.084039</td>\n","      <td>0.038577</td>\n","      <td>0.655305</td>\n","      <td>0.002930</td>\n","      <td>0.832237</td>\n","      <td>0.933127</td>\n","      <td>0.646431</td>\n","      <td>0.979862</td>\n","      <td>0.946747</td>\n","      <td>0.730714</td>\n","      <td>0.040916</td>\n","      <td>0.015051</td>\n","      <td>0.853433</td>\n","      <td>0.913076</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>46</td>\n","      <td>0.852433</td>\n","      <td>0.395800</td>\n","      <td>0.004266</td>\n","      <td>0.820851</td>\n","      <td>0.694166</td>\n","      <td>0.943101</td>\n","      <td>0.580638</td>\n","      <td>0.485877</td>\n","      <td>0.019464</td>\n","      <td>0.006713</td>\n","      <td>0.423158</td>\n","      <td>0.118904</td>\n","      <td>0.001748</td>\n","      <td>0.001615</td>\n","      <td>0.001691</td>\n","      <td>0.002816</td>\n","      <td>0.914795</td>\n","      <td>0.052117</td>\n","      <td>0.054587</td>\n","      <td>0.000704</td>\n","      <td>0.499841</td>\n","      <td>0.950354</td>\n","      <td>0.667901</td>\n","      <td>0.982334</td>\n","      <td>0.981792</td>\n","      <td>0.893747</td>\n","      <td>0.966207</td>\n","      <td>0.049795</td>\n","      <td>0.029797</td>\n","      <td>0.919274</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>70</td>\n","      <td>0.842327</td>\n","      <td>0.506961</td>\n","      <td>0.004563</td>\n","      <td>0.883937</td>\n","      <td>0.932034</td>\n","      <td>0.988546</td>\n","      <td>0.683724</td>\n","      <td>0.545802</td>\n","      <td>0.043648</td>\n","      <td>0.004434</td>\n","      <td>0.114470</td>\n","      <td>0.761507</td>\n","      <td>0.004864</td>\n","      <td>0.022064</td>\n","      <td>0.003058</td>\n","      <td>0.002728</td>\n","      <td>0.054134</td>\n","      <td>0.014959</td>\n","      <td>0.755193</td>\n","      <td>0.001444</td>\n","      <td>0.723875</td>\n","      <td>0.945495</td>\n","      <td>0.616236</td>\n","      <td>0.971957</td>\n","      <td>0.981394</td>\n","      <td>0.751724</td>\n","      <td>0.079267</td>\n","      <td>0.034405</td>\n","      <td>0.811195</td>\n","      <td>0.902751</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>132</td>\n","      <td>0.882259</td>\n","      <td>0.339376</td>\n","      <td>0.004279</td>\n","      <td>0.635084</td>\n","      <td>0.730497</td>\n","      <td>0.905576</td>\n","      <td>0.589358</td>\n","      <td>0.443165</td>\n","      <td>0.073747</td>\n","      <td>0.011523</td>\n","      <td>0.526853</td>\n","      <td>0.013763</td>\n","      <td>0.002484</td>\n","      <td>0.006580</td>\n","      <td>0.002225</td>\n","      <td>0.001478</td>\n","      <td>0.930140</td>\n","      <td>0.073568</td>\n","      <td>0.602392</td>\n","      <td>0.000985</td>\n","      <td>0.725173</td>\n","      <td>0.925942</td>\n","      <td>0.674307</td>\n","      <td>0.961560</td>\n","      <td>0.980880</td>\n","      <td>0.875148</td>\n","      <td>0.924776</td>\n","      <td>0.073127</td>\n","      <td>0.500388</td>\n","      <td>0.914971</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>200</td>\n","      <td>0.872123</td>\n","      <td>0.286056</td>\n","      <td>0.008000</td>\n","      <td>0.878443</td>\n","      <td>0.790840</td>\n","      <td>0.967963</td>\n","      <td>0.666880</td>\n","      <td>0.658153</td>\n","      <td>0.022098</td>\n","      <td>0.007939</td>\n","      <td>0.266337</td>\n","      <td>0.121490</td>\n","      <td>0.003425</td>\n","      <td>0.006583</td>\n","      <td>0.005576</td>\n","      <td>0.024539</td>\n","      <td>0.229945</td>\n","      <td>0.082714</td>\n","      <td>0.453629</td>\n","      <td>0.001472</td>\n","      <td>0.465335</td>\n","      <td>0.943488</td>\n","      <td>0.666579</td>\n","      <td>0.979266</td>\n","      <td>0.958229</td>\n","      <td>0.826378</td>\n","      <td>0.330356</td>\n","      <td>0.141307</td>\n","      <td>0.694931</td>\n","      <td>0.894270</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   qa_id  ...  answer_well_written\n","0     39  ...             0.913076\n","1     46  ...             0.919274\n","2     70  ...             0.902751\n","3    132  ...             0.914971\n","4    200  ...             0.894270\n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"-Y51i5_yF0sl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}